#Loading data into a dataframe
#Product file example (comma delimited)

ProductID,ProductName,Category,ListPrice
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...

#In a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows
#The %%pyspark line at the beginning is called a magic

%%pyspark
df = spark.read.load('abfss://container@store.dfs.core.windows.net/products.csv',
    format='csv',
    header=True
)
display(df.limit(10))

#here's the equivalent Scala code for the products data example

%%spark
val df = spark.read.format("csv").option("header", "true").load("abfss://container@store.dfs.core.windows.net/products.csv")
display(df.limit(10))

#Specifying a dataframe schema

#You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...

#The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format
from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
    StructField("ProductID", IntegerType()),
    StructField("ProductName", StringType()),
    StructField("Category", StringType()),
    StructField("ListPrice", FloatType())
    ])

df = spark.read.load('abfss://container@store.dfs.core.windows.net/product-data.csv',
    format='csv',
    schema=productSchema,
    header=False)
display(df.limit(10))

#Filtering and grouping dataframes

#You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. 
#For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example

pricelist_df = df.select("ProductID", "ListPrice")

#Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:

pricelist_df = df["ProductID", "ListPrice"]

#You can "chain" methods together to perform a series of manipulations that results in a transformed dataframe
# For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:

bikes_df = df.select("ProductName", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes"))
display(bikes_df)

#To group and aggregate data, you can use the groupBy method and aggregate functions

#For example, the following PySpark code counts the number of products for each category:

counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)

#Creating database objects in the Spark catalog

#One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example
df.createOrReplaceTempView("products")
#A view is temporary, meaning that it's automatically deleted at the end of the current session

#You can create an empty table by using the spark.catalog.createTable method
#You can save a dataframe as a table by using its saveAsTable method
#You can create an external table by using the spark.catalog.createExternalTable method

#Using the Spark SQL API to query data

#You can use the Spark SQL API in code written in any language to query data in the catalog
#For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe

bikes_df = spark.sql("SELECT ProductID, ProductName, ListPrice \
                      FROM products \
                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')")
display(bikes_df)

#Using SQL Code

#In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this:

%%sql

SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category

#Visualize data with Spark

#Using graphics packages in code
#For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.

from matplotlib import pyplot as plt

# Get the data as a Pandas dataframe
data = spark.sql("SELECT Category, COUNT(ProductID) AS ProductCount \
                  FROM products \
                  GROUP BY Category \
                  ORDER BY Category").toPandas()

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(12,8))

# Create a bar plot of product counts by category
plt.bar(x=data['Category'], height=data['ProductCount'], color='orange')

# Customize the chart
plt.title('Product Counts by Category')
plt.xlabel('Category')
plt.ylabel('Products')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=70)

# Show the plot area
plt.show()
